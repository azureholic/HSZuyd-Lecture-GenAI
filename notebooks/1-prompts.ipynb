{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inititalize the notebook\n",
    "import os\n",
    "import textwrap\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version = os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
    ")\n",
    "\n",
    "chat_deployment_name = \"gpt-4\"\n",
    "\n",
    "print(\"*** init done! ***\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Hoe lang duurt een HBO-ICT opleiding in Heerlen?\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model = chat_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "lines = textwrap.wrap(response.choices[0].message.content, width=80)\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Hoe lang duurt een HBO-ICT opleiding in Heerlen?\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model = chat_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : \"Je bent een behulpzame assistent die antwoord als een grappige kat\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "lines = textwrap.wrap(response.choices[0].message.content, width=80)\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESET THE SYSTEM PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstQuestion = \"Hoe lang duurt een HBO-ICT opleiding in Heerlen?\"\n",
    "\n",
    "firstResponse = openai_client.chat.completions.create(\n",
    "    model = chat_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": firstQuestion\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : \"Je bent een behulpzame assistent\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "lines = textwrap.wrap(firstResponse.choices[0].message.content, width=80)\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vervolgvraag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondQuestion = \"Wat is het adres van de campus?\"\n",
    "\n",
    "secondResponse = openai_client.chat.completions.create(\n",
    "    model = chat_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": secondQuestion\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : \"Je bent een behulpzame assistent\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "lines = textwrap.wrap(secondResponse.choices[0].message.content, width=80)\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemWithMemory = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": f\"\"\"\n",
    "            use the previous question and answer as a context \n",
    "            ##\n",
    "            previous question {firstQuestion}\n",
    "            previous answer {firstResponse.choices[0].message.content}\n",
    "            \"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "secondResponse = openai_client.chat.completions.create(\n",
    "    model = chat_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": secondQuestion\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : \"Je bent een behulpzame assistent\"\n",
    "        },\n",
    "        systemWithMemory\n",
    "    ]\n",
    ")\n",
    "\n",
    "lines = textwrap.wrap(secondResponse.choices[0].message.content, width=80)\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "short term memory - langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=chat_deployment_name,\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_API_BASE\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    openai_api_type=\"azure\",\n",
    ") \n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"Je bent een behulpzame assistent\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(llm=llm, prompt=prompt, verbose=False, memory=memory)\n",
    "\n",
    "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
    "print(\"---------\")\n",
    "print(firstQuestion)\n",
    "print(\"---------\")\n",
    "response = conversation({\"question\": firstQuestion})\n",
    "\n",
    "lines = textwrap.wrap(response[\"text\"], width=80)\n",
    "for line in lines:\n",
    "    print(line)\n",
    "\n",
    "\n",
    "print(\"---------\")\n",
    "print(secondQuestion + \" (nu hebben we het antwoord van de eerste vraag als context)\")\n",
    "print(\"---------\")\n",
    "response = conversation({\"question\": secondQuestion})\n",
    "\n",
    "lines = textwrap.wrap(response[\"text\"], width=80)\n",
    "for line in lines:\n",
    "    print(line)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kunt nu gewoon verder chatten, memory wordt steeds verder uitgebreid\n",
    "\n",
    "Let op! Je prompt wordt steeds groter, dus je verbruikt meer tokens\n",
    "Wellicht heb je niet alle historie van de chat nodig.\n",
    "\n",
    "dit is ook een voorbeeld hoe je een specifieke instructie geeft. \"antwoord alleen met de link naar Google Maps\" is een hele specifieke instructie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdQuestion = \"\"\"\n",
    "                    hoe zou een link naar Google Maps eruit zien voor dit adres?\n",
    "                    antwoord alleen met de link naar Google Maps\n",
    "                \"\"\"\n",
    "print(\"---------\")\n",
    "print(thirdQuestion)\n",
    "print(\"---------\")\n",
    "response = conversation({\"question\": thirdQuestion})\n",
    "\n",
    "lines = textwrap.wrap(response[\"text\"], width=80)\n",
    "for line in lines:\n",
    "    print(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nog een voorbeeld van een hele specifieke instructie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JsonQuestion = \"\"\"\n",
    "                    format your output in a json format\n",
    "                    output a plain json as provided in the example with the address\n",
    "                    ## sample json\n",
    "                    {\n",
    "                        \"street\": \"street\",\n",
    "                        \"city\": \"city\",\n",
    "                        \"country\": \"country\",\n",
    "                        \"postal_code\": \"postal_code\"\n",
    "                    }\n",
    "                                        \n",
    "                \"\"\"\n",
    "print(\"---------\")\n",
    "print(JsonQuestion)\n",
    "print(\"---------\")\n",
    "response = conversation({\"question\": JsonQuestion})\n",
    "\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een hele specifieke vraag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySpecificQuestion = \"wanneer is de open dag?\"\n",
    "\n",
    "mySpecificResponse = openai_client.chat.completions.create(\n",
    "    model = chat_deployment_name,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": mySpecificQuestion\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : \"Je bent een behulpzame assistent\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "lines = textwrap.wrap(mySpecificResponse.choices[0].message.content, width=80)\n",
    "for line in lines:\n",
    "    print(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wellicht kun je wat meer context geven in de prompt.\n",
    "\n",
    "We gaan nu data toevoegen, hele specifieke data, namelijk een aantal pagina's\n",
    "van de HSZuyd Website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lees de pagina's om deze later toe te voegen in de prompt\n",
    "page1 = open(\"../docs/page-0.md\", \"r\").read()\n",
    "page2 = open(\"../docs/page-1.md\", \"r\").read()\n",
    "page3 = open(\"../docs/page-2.md\", \"r\").read()\n",
    "\n",
    "\n",
    "myDataQuestion = \"wanneer is de open dag?\"\n",
    "\n",
    "myDataResponse = openai_client.chat.completions.create(\n",
    "    model = chat_deployment_name,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": myDataQuestion\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : f\"\"\"\n",
    "                Je bent een behulpzame assistent\n",
    "                Je geeft alleen antwoord op basis van de data die je hebt gekregen\n",
    "                Indien je het antwoord niet kunt vinden, \n",
    "                zeg je dat je het antwoord niet weet\n",
    "                \n",
    "                ## data\n",
    "                {page1}\n",
    "\n",
    "                ## data\n",
    "                {page2}\n",
    "\n",
    "                ## data\n",
    "                {page3}\n",
    "\n",
    "                \"\"\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "lines = textwrap.wrap(myDataResponse.choices[0].message.content, width=80)\n",
    "for line in lines:\n",
    "    print(line)\n",
    "\n",
    "print(\"----\")\n",
    "print(\"Hier wat info over de tokens die je hebt gebruikt:\")\n",
    "print(myDataResponse.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay....dit lijkt werken, maar we verbruiken heel veel tokens...tijd voor een volgende stap.\n",
    "\n",
    "***Embeddings en vectoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
